# -*- coding: utf-8 -*-
"""Copy of support_vector_regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1R_mO8fK5egLqHdgSGIYrCq7OoYzxmLid

# Support Vector Regression (SVR)

## Importing the libraries

Here, we import NumPy, scikit-learn, and Matplotlib to make our code shorter and more efficient!
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

"""## Importing the dataset

The dataset provides a series of positions, along with the corresponding salaries. The "iloc" method is used to select the rows of data.
"""

dataset = pd.read_csv('Position_Salaries.csv')
X = dataset.iloc[:, 1:-1].values
Y = dataset.iloc[:, -1].values

print(X)

print(Y)

"""## Feature Scaling

Here, the data is scaled to ensure that salaries feature is interpreted on the same scale as the independent variable, the level.
"""

Y = Y.reshape(len(Y),1)

print(Y)

from sklearn.preprocessing import StandardScaler
sc_X = StandardScaler()
sc_Y = StandardScaler()
X = sc_X.fit_transform(X)
Y = sc_Y.fit_transform(Y)

print(X)

print(Y)

"""## Training the SVR model on the whole dataset

Here, we train the support vector regression model on the whole dataset. The kernel type is set to radial basis function (rbf), which is what determines how the data can be separated.
"""

from sklearn.svm import SVR
regressor = SVR(kernel = 'rbf')
regressor.fit(X, Y)

"""## Predicting a new result

Here, we predict the salary at position level 6.5. The inverse transform method is used to transform the scaled value back to a salary value!
"""

sc_Y.inverse_transform(regressor.predict(sc_X.transform([[6.5]])).reshape(-1,1))

"""## Visualising the SVR results

Here, the results of the model are visualised for ease of understanding. To create the visual, we use Matplotlib.
"""

plt.scatter(sc_X.inverse_transform(X), sc_Y.inverse_transform(Y), color = 'red')
plt.plot(sc_X.inverse_transform(X), sc_Y.inverse_transform(regressor.predict(X).reshape(-1,1)), color = 'blue')
plt.title('Truth or Bluff (SVR)')
plt.xlabel('Position Level')
plt.ylabel('Salary')
plt.show()

"""## Visualising the SVR results (for higher resolution and smoother curve)

Here, the model is calculated in increments of 0.1 instead of 1 to have a smoother and more accurate curve.
"""

X_grid = np.arange(min(sc_X.inverse_transform(X)), max(sc_X.inverse_transform(X)), 0.1)
X_grid = X_grid.reshape((len(X_grid), 1))
plt.scatter(sc_X.inverse_transform(X), sc_Y.inverse_transform(Y), color = 'red')
plt.plot(X_grid, sc_Y.inverse_transform(regressor.predict(sc_X.transform(X_grid)).reshape(-1,1)), color = 'blue')
plt.title('Truth or Bluff (SVR)')
plt.xlabel('Position level')
plt.ylabel('Salary')
plt.show()

"""## Finding the R-squared Value

Here, we find the R-squared value for the model on the test data, indicating how well the model fits the data.

An R-squared value ranges from 0-1, with a higher value indicating a stronger fit.

This will be completed with various regression models, to see which one works best for our dataset!
"""

from sklearn.metrics import r2_score
Y_pred = regressor.predict(X)
Y_pred = sc_Y.inverse_transform(Y_pred.reshape(-1, 1))
r_squared = r2_score(sc_Y.inverse_transform(Y), Y_pred)
print(f"R-squared value: {r_squared}")